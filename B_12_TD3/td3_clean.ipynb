{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TD3 Notebook (CleanRL)** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some changes in vairable names: \n",
    "- qf1_a_values -> q_est1\n",
    "- qf1_loss -> q_loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (A/B) Set UP ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Import ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B2) Args ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(dict):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super(Args, self).__init__(*args, **kwargs)\n",
    "            self.__dict__ = self\n",
    "\n",
    "args = {\n",
    "    \"algo\": \"td3\", \n",
    "    \"env_id\": \"Hopper-v4\", #\"MountainCarContinuous-v0\", #\n",
    "    \"seed\": 1, \n",
    "\n",
    "    \"total_timesteps\": 800_000,\n",
    "    \"gamma\": 0.99, \n",
    "    \"learning_rate\": 3e-4, # paper uses 1e-3, CleanRL uses 3e-4\n",
    "    \"buffer_size\": int(1e6), # paper says buffer contains entire history, but code on Github has changed\n",
    "    \"batch_size\": 256, \n",
    "    \"tau\": 0.005, # target smoothing coefficient \n",
    "    \"policy_noise\": 0.2, # noise in target policy smoothing\n",
    "    \"exploration_noise\": 0.1, # multiplied to a normally distributed noise that is added to actor's action\n",
    "    \"learning_starts\": int(25e3), \n",
    "    \"policy_update_interval\": 2, # ?really? 2 and 2 have so much of a difference\n",
    "    \"noise_clip\": 0.5, \n",
    "    # my addition\n",
    "    \"num_envs\": int(1), \n",
    "\n",
    "    \"torch_deterministic\": True, \n",
    "    \"cuda\": True, \n",
    "\n",
    "    \"capture_video\": True,\n",
    "}\n",
    "\n",
    "args = Args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "run_name = f\"{args.env_id}__{args.algo}__{args.seed}__{start_datetime}\"\n",
    "\n",
    "print(f\"start_datetime = {start_datetime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B3) Hardware ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"device_name = {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B4) Tensorboard ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\", \n",
    "    \"|param|value|\\n|-|-|\\n%s\" % \"\\n\".join(f\"|{key}|{val}\" for key, val in args.items())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B5) Seeding ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False #\n",
    "\n",
    "# result not reproducible! (bc stupid thing was using sb3 kernel?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Implementation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C1) env and vec_env ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if idx == 0 and capture_video:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"runs/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(args.env_id, args.seed, idx, args.capture_video, run_name) for idx in range(args.num_envs)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C2) Agent ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = 256\n",
    "        self.fc1 = nn.Linear(\n",
    "                np.prod(envs.single_observation_space.shape) + np.prod(envs.single_action_space.shape), \n",
    "                hidden_size\n",
    "        )\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        q = torch.cat([s, a], 1)\n",
    "        q = F.relu(self.fc1(q))\n",
    "        q = F.relu(self.fc2(q))\n",
    "        q = self.fc3(q)\n",
    "        return q\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = 256\n",
    "        self.fc1 = nn.Linear(np.prod(envs.single_observation_space.shape), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, np.prod(envs.single_action_space.shape))\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", \n",
    "            torch.tensor((envs.action_space.high - envs.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", \n",
    "            torch.tensor((envs.action_space.high + envs.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "    \n",
    "    def forward(self, s):\n",
    "        a = F.relu(self.fc1(s))\n",
    "        a = F.relu(self.fc2(a))\n",
    "        a = torch.tanh(self.fc3(a))\n",
    "        return a * self.action_scale + self.action_bias\n",
    "\n",
    "# I changed env in __init__ to envs; non-vector env does not have .single_observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C3) Training ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (C3a) Init ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(envs).to(device)\n",
    "actor_target = Actor(envs).to(device)\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic1 = Critic(envs).to(device)\n",
    "critic2 = Critic(envs).to(device)\n",
    "critic1_target = Critic(envs).to(device)\n",
    "critic2_target = Critic(envs).to(device)\n",
    "critic1_target.load_state_dict(critic1.state_dict())\n",
    "critic2_target.load_state_dict(critic2.state_dict())\n",
    "\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=args.learning_rate)\n",
    "optimizer_critic = optim.Adam(list(critic1.parameters()) + list(critic2.parameters()), lr=args.learning_rate)\n",
    "\n",
    "# Mujoco uses float32 action space and float 64 obs space\n",
    "# but envs.observation_space.dtype still = np.float64\n",
    "# ; however, if this line is before ReplayBuffer construction, rb.sample() yields float32 transitions (??)\n",
    "envs.single_observation_space.dtype = np.float32 \n",
    "# envs.observation_space.dtype = np.float32 # does not work, \n",
    "# e.g. envs.observation_space.sample() raises AttributeError: type object 'numpy.float32' has no attribute 'kind'\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size, \n",
    "    envs.single_observation_space, \n",
    "    envs.single_action_space, \n",
    "    device, \n",
    "    handle_timeout_termination=False, # (I think) only usable with SB3's API\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning sb3.ReplayBuffer\n",
    "\"\"\"\n",
    "obs, _ = envs.reset()\n",
    "# step #1\n",
    "actions = envs.action_space.sample()\n",
    "obs1, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "rb.add(obs, obs1, actions, rewards, terminations, infos)\n",
    "# step #2\n",
    "actions = envs.action_space.sample()\n",
    "obs1, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "rb.add(obs, obs1, actions, rewards, terminations, infos)\n",
    "\n",
    "rb.sample(2) # uses np.random.randint, draw WITH replacement (2024/06)\n",
    "rb.sample(2).observations # 2 tensors\n",
    "rb.sample(2).observations[0].size() # torch.Size([11])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (C3b) Training Loop ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "global_episode = 0\n",
    "global_step = 0\n",
    "# can store reusing tensor here. clamp tensors for actions1\n",
    "\n",
    "obs, _ = envs.reset()\n",
    "while global_step < args.total_timesteps:\n",
    "    # 1) pick action\n",
    "    if global_step < args.learning_starts:\n",
    "        actions = envs.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            actions = actor(torch.Tensor(obs).to(device))\n",
    "            actions += torch.normal(0, actor.action_scale * args.exploration_noise)\n",
    "            actions = actions.cpu().numpy().clip(envs.action_space.low, envs.action_space.high)\n",
    "\n",
    "    # 2) step\n",
    "    obs1, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "    # 3) store transition in replay buffer\n",
    "    rb.add(obs, obs1, actions, rewards, terminations, infos)\n",
    "\n",
    "    # 4) rollout to next step\n",
    "    obs = obs1\n",
    "    global_step += envs.num_envs\n",
    "    if \"final_info\" in infos:\n",
    "        written = False\n",
    "        for info in infos[\"final_info\"]:\n",
    "            if info:\n",
    "                global_episode += 1\n",
    "                if not written:\n",
    "                    writer.add_scalar(\"charts/episodic_reward\", info[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "                    written = True\n",
    "        writer.add_scalar(\"charts/episode\", global_episode, global_step)\n",
    "\n",
    "    # 5) training\n",
    "    # 5a) update critics\n",
    "    if global_step > args.learning_starts:\n",
    "        batch = rb.sample(args.batch_size)\n",
    "        with torch.no_grad(): # update critics ignore actor / td_target as constant\n",
    "            actions1 = actor_target(batch.next_observations)\n",
    "            # CleanRL's noise: (N(0,1) * policy_noise).clamp(-c, c) <- using this (paper's code also use this)\n",
    "            # paper's noise: N(0,σ).clamp(-c, c)\n",
    "            actions1_noise = (torch.randn_like(batch.actions, device=device) * args.policy_noise)\\\n",
    "                    .clamp(-args.noise_clip, args.noise_clip) * actor_target.action_scale\n",
    "            actions1 = (actions1 + actions1_noise).clamp(\n",
    "                torch.tensor(envs.single_action_space.low).reshape((1,-1)).to(device), \n",
    "                torch.tensor(envs.single_action_space.high).reshape((1,-1)).to(device)\n",
    "            ) # cleanRL uses aingle_action_space.low[0] ?why the sudden drop of generality?\n",
    "            q1 = critic1_target(batch.next_observations, actions1)\n",
    "            q2 = critic2_target(batch.next_observations, actions1)\n",
    "            qmin = torch.min(q1, q2)\n",
    "            td_target = (batch.rewards + (1. - batch.dones) * args.gamma * qmin).flatten() # my timeit shows flatten afterwards slightly faster\n",
    "        \n",
    "        q_est1 = critic1(batch.observations, batch.actions).flatten()\n",
    "        q_est2 = critic2(batch.observations, batch.actions).flatten()\n",
    "        q_loss1 = F.mse_loss(td_target, q_est1)\n",
    "        q_loss2 = F.mse_loss(td_target, q_est2)\n",
    "        q_loss = q_loss1 + q_loss2 # so critic1 and critic2 are only differed by initiation (?)\n",
    "        # They are very close to one another on tensorboard (<10% I guess), but maybe that is sufficient to suppress the drift\n",
    "\n",
    "        optimizer_critic.zero_grad()\n",
    "        q_loss.backward()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        if global_step % args.policy_update_interval == 0:\n",
    "            # 5b) update actor\n",
    "            actor_loss = -critic1(batch.observations, actor(batch.observations)).mean()\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "\n",
    "            # 5c) update targets\n",
    "            for param, target_param in zip(actor.parameters(), actor_target.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    args.tau * param.data + (1-args.tau) * target_param.data\n",
    "                )\n",
    "            for param, target_param in zip(critic1.parameters(), critic1_target.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    args.tau * param.data + (1-args.tau) * target_param.data\n",
    "                )\n",
    "            for param, target_param in zip(critic2.parameters(), critic2_target.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    args.tau * param.data + (1-args.tau) * target_param.data\n",
    "                )\n",
    "\n",
    "        # 6) progress tracking\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalars(\"losses/q_est\", {\n",
    "                \"q_est1\": q_est1.mean().item(),\n",
    "                \"q_est2\": q_est2.mean().item()\n",
    "            }, global_step)\n",
    "            writer.add_scalars(\"losses/q_loss\", {\n",
    "                \"q_loss1\": q_loss1.item(),\n",
    "                \"q_loss2\": q_loss2.item()\n",
    "            }, global_step)\n",
    "            writer.add_scalar(\"losses/q_loss\", q_loss.item(), global_step)\n",
    "            writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(f\"upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Evaluation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, n_eval_episodes, policy, hyperparameters=args):\n",
    "    # (1) evaluate\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                state = torch.Tensor(state).to(device)\n",
    "                action = policy(state)\n",
    "                new_state, reward, terminated, truncated, info = env.step(action.flatten().cpu().numpy())\n",
    "                total_rewards_ep += reward\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "                state = new_state\n",
    "            episode_rewards.append(total_rewards_ep)\n",
    "            print(f\"episode {episode:2}: reward={total_rewards_ep:5.1f}\")\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    # (2) metadata\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "    evaluate_data = {\n",
    "        \"env_id\": hyperparameters.env_id,\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"std_reward\": std_reward,\n",
    "        \"n_evaluation_episodes\": n_eval_episodes,\n",
    "        \"eval_datetime\": eval_form_datetime,\n",
    "    }\n",
    "\n",
    "    return mean_reward, std_reward, evaluate_data\n",
    "\n",
    "eval_env = gym.make(args.env_id, render_mode=\"rgb_array\")\n",
    "eval_env = gym.wrappers.RecordVideo(eval_env, video_folder=\"upload\", video_length=99999)\n",
    "\n",
    "eval_mean_reward, eval_std_reward, evaluate_data = evaluate_agent(eval_env, 10, actor)\n",
    "eval_env.close()\n",
    "print(f\"mean_reward={eval_mean_reward:.2f}\")\n",
    "print(f\"std_reward={eval_std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (E) Save ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(f\"runs/{run_name}/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), f\"runs/{run_name}/models/actor.pt\")\n",
    "torch.save(critic1.state_dict(), f\"runs/{run_name}/models/critic1.pt\")\n",
    "torch.save(critic2.state_dict(), f\"runs/{run_name}/models/critic2.pt\")\n",
    "f\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "t1 = torch.tensor([[.1,.2,.3,.4]], dtype=torch.float32, device=device)\n",
    "t2 = torch.tensor([[.4,.3,.2,.1]], dtype=torch.float32, device=device)\n",
    "t3 = torch.tensor([[.5,.6,.7,.8]], dtype=torch.float32, device=device)\n",
    "\n",
    "%timeit -n 100_000 -r 5 t1 + t2 + t3 * 2.0\n",
    "# 31.9 µs ± 2.13 µs per loop\n",
    "# 28.5 µs ± 945 ns per loop\n",
    "\n",
    "%timeit -n 100_000 -r 5 t1.flatten() + t2.flatten() + t3.flatten() * 2.0\n",
    "# 38.5 µs ± 969 ns per loop\n",
    "# 39.2 µs ± 1.05 µs per loop\n",
    "\n",
    "%timeit -n 100_000 -r 5 (t1 + t2 + t3 * 2.0).flatten()\n",
    "# 36.1 µs ± 2.16 µs per loop\n",
    "# 36.3 µs ± 1.59 µs per loop\n",
    "\n",
    "%timeit -n 100_000 -r 5 (t1 + t2 + t3 * 2.0).view(-1)\n",
    "# 35.9 µs ± 1.23 µs per loop\n",
    "# 34.8 µs ± 1.6 µs per loop\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
