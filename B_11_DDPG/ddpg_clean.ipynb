{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DDPG Notebook (CleanRL)** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is modifed from CleanRL https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ddpg_continuous_action.py#L170\n",
    "\n",
    "Notable modifications:\n",
    "\n",
    "Variables renamed:\n",
    "- qf1 -> qn\n",
    "- qf1_next_target -> q1\n",
    "- next_q_value -> td_targets\n",
    "- qf1_a_values -> q_est\n",
    "- qf1_loss -> q_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (A/B) setup ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B2) args ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(dict): # https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute?page=1&tab=scoredesc#tab-top\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Args, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "args = {\n",
    "    \"algo\": \"DDPG\", \n",
    "    \"seed\": 1,\n",
    "    \"capture_video\": True, \n",
    "    # algorithm-specific\n",
    "    \"env_id\": \"Hopper-v4\", #\"MountainCarContinuous-v0\", #\n",
    "    \"total_timesteps\": 1_000_000, \n",
    "    \"learning_rate\": 3e-4, \n",
    "    \"buffer_size\": int(1e6), \n",
    "    \"gamma\": 0.99, \n",
    "    \"tau\": 0.005, # target smoothing coefficient \n",
    "        # (how much target netowrks moves towards online network in an update; \n",
    "        # note target_param updates every policy_update_interval\n",
    "    \"batch_size\": 256, \n",
    "    \"exploration_noise\": 0.1, # exploration_noise * actor.action_scale is added to actor(obs) for exploration, see (C3) Training\n",
    "    \"learning_starts\": 25e3, \n",
    "    \"policy_update_interval\": 2, # Q-netowrk (ie. critics) updates every step, policy (ie. actor) updates every this-many steps\n",
    "\n",
    "    \"torch_deterministic\": True, \n",
    "    \"cuda\": True, \n",
    "}\n",
    "\n",
    "args = Args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "run_name = f\"{args.env_id}__{args.algo}__{args.seed}__{start_datetime}\"\n",
    "\n",
    "print(f\"start_datetime = {start_datetime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B3) Hardware ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"device_name = {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B4) Tensorboard ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\", \n",
    "    \"|param|value|\\n|-|-|\\n%s\" % \"\\n\".join(f\"|{key}|{val}\" for key, val in args.items())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B5) Seeding ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Implementation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C1) env and vec_env ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_recorded_step = 0\n",
    "record_interval = 10_000\n",
    "def record_video_step_trigger(step):\n",
    "    global last_recorded_step\n",
    "    if step - last_recorded_step >= record_interval:\n",
    "        last_recorded_step += record_interval\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def record_video_ep_trigger(episode):\n",
    "    if episode % 20 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, capture_video, run_name, step_trigger=None):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\", step_trigger=record_video_step_trigger, video_length=1500)\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed) # this seed is used for env.action_space.sample()\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike PPO, DDPG uses a single environment\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, args.seed, 0, args.capture_video, run_name)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C2) Agent ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        hidden_size = 256\n",
    "        # the network takes a state-action(s) pair and output its Q value\n",
    "        self.fc1 = nn.Linear(\n",
    "                np.prod(env.single_observation_space.shape) + np.prod(env.single_action_space.shape),\n",
    "                hidden_size\n",
    "        )\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1) # concat state and action, see __init__\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        hidden_size = 256\n",
    "        # Takes in state only (unlike the critic) and output an action tensor\n",
    "        self.fc1 = nn.Linear(np.prod(env.single_observation_space.shape), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_mu = nn.Linear(hidden_size, np.prod(env.single_action_space.shape)) # what is 'mu'?\n",
    "\n",
    "        # action rescaling , register_buffer https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): #note that x is NOT normalized\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C3) Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(envs).to(device)\n",
    "actor_target = Actor(envs).to(device)\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "\n",
    "qn = QNetwork(envs).to(device)\n",
    "qn_target = QNetwork(envs).to(device)\n",
    "qn_target.load_state_dict(qn.state_dict())\n",
    "\n",
    "optimizer_q = optim.Adam(qn.parameters(), lr=args.learning_rate)\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=args.learning_rate)\n",
    "\n",
    "envs.single_observation_space.dtype = np.float32\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size, \n",
    "    envs.single_observation_space, \n",
    "    envs.single_action_space, \n",
    "    device, \n",
    "    handle_timeout_termination=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "episode_now = 0\n",
    "\n",
    "obs, _ = envs.reset(seed=args.seed)\n",
    "for global_step in range(args.total_timesteps):\n",
    "    # 1) pick action\n",
    "    if global_step < args.learning_starts:\n",
    "        # 1a) haven't learned, init random ~ other random\n",
    "        actions = envs.action_space.sample()\n",
    "    else:\n",
    "        # 1b) according to actor, with exploration_noise\n",
    "        with torch.no_grad():\n",
    "            actions = actor(torch.Tensor(obs).to(device))\n",
    "            actions += torch.normal(0, actor.action_scale * args.exploration_noise) # set mean for SDE\n",
    "            actions = actions.cpu().numpy().clip(envs.single_action_space.low, envs.single_action_space.high)\n",
    "    \n",
    "    # 2) step\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "    # how termination and truncation are handled: \n",
    "    # termination becomes done in the replay_buffer, and if done, we do not bootstrap next state when computing td_target\n",
    "    # truncation is treated identically to non-truncated states. This implementation only undo the auto-reset and put the transitioned state instead of the reset state into the replay_buffer\n",
    "    \n",
    "    # next obs undoing the auto-reset of vec_env.step(), I named it transitional because reset is not transitional\n",
    "    next_obs_transitional = next_obs.copy()\n",
    "    for idx, trunc in enumerate(truncations):\n",
    "        if trunc:\n",
    "            next_obs_transitional[idx] = infos[\"final_observation\"][idx]\n",
    "\n",
    "    # print and log training progress\n",
    "    if \"final_info\" in infos:\n",
    "        for info in infos[\"final_info\"]:\n",
    "            if info is None:\n",
    "                continue\n",
    "            print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            break # even if two envs are done at the same step, showing one is enough\n",
    "\n",
    "    # 3) save data to replay buffer\n",
    "    rb.add(obs, next_obs_transitional, actions, rewards, terminations, infos) # no truncation because replay_buffer.handle_timeout_termination = False? \n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "    # 4) updates\n",
    "    if global_step > args.learning_starts:\n",
    "        # 4A) update Q-network\n",
    "        batch = rb.sample(args.batch_size) # is a `ReplayBufferSamples`, API similar to ReplayBuffer(?)\n",
    "        # td-target q(s,a) = r + γ * q1(s1,a1)\n",
    "        with torch.no_grad():\n",
    "            action1 = actor_target(batch.next_observations)\n",
    "            q1 = qn_target(batch.next_observations, action1)\n",
    "            td_targets = batch.rewards.flatten() + (1 - batch.dones.flatten()) * args.gamma * q1.view(-1)\n",
    "        # td estimation\n",
    "        q_est = qn(batch.observations, batch.actions).view(-1)\n",
    "        q_loss = F.mse_loss(td_targets, q_est)\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer_q.zero_grad()\n",
    "        q_loss.backward()\n",
    "        optimizer_q.step()\n",
    "\n",
    "        # 4B) update actor network\n",
    "        if global_step % args.policy_update_interval == 0:\n",
    "            # maximizing Q w.r.t. actor.parameters()\n",
    "            actor_loss = -qn(batch.observations, actor(batch.observations)).mean() # DDPG article eq(6), note here we are using optimizer_actor.step(), μ() in the article is just actor policy\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "\n",
    "            # 4C) update target networks (note how this is in `% policy_update_interval == 0`)\n",
    "            # a param is a tensor of a layer's weights or biases, so there are only 2 x (n+1) param for a model which has n hidden layers\n",
    "            for param, target_param in zip(actor.parameters(), actor_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qn.parameters(), qn_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "        # 5) learn progress tracking\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalar(\"losses/q_est\", q_est.mean().item(), global_step) # why is this informative, esp when there is only one env\n",
    "            # when QNetwork is learned, q_est will be smooth\n",
    "            # also monitor over-optimism\n",
    "            writer.add_scalar(\"losses/q_loss\", q_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "            sps = global_step / (time.time() - start_time) # step per sec\n",
    "            writer.add_scalar(\"charts/SPS\", int(sps), global_step)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Evaluation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, n_eval_episodes, policy, hyperparameters=args):\n",
    "    # (1) evaluate\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while True:\n",
    "                state = torch.Tensor(state).to(device)\n",
    "                action = policy(state)\n",
    "                new_state, reward, terminated, truncated, info = env.step(action.flatten().cpu().numpy())\n",
    "                total_rewards_ep += reward\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "                state = new_state\n",
    "            episode_rewards.append(total_rewards_ep)\n",
    "            print(f\"episode {episode:2}: reward={total_rewards_ep:5.1f}\")\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    # (2) metadata\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "    evaluate_data = {\n",
    "        \"env_id\": hyperparameters.env_id,\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"std_reward\": std_reward,\n",
    "        \"n_evaluation_episodes\": n_eval_episodes,\n",
    "        \"eval_datetime\": eval_form_datetime,\n",
    "    }\n",
    "\n",
    "    return mean_reward, std_reward, evaluate_data\n",
    "\n",
    "eval_env = gym.make(args.env_id, render_mode=\"rgb_array\")\n",
    "eval_env = gym.wrappers.RecordVideo(eval_env, video_folder=\"upload\", video_length=99999) # 1000=20sec (not 30FPS?)\n",
    "\n",
    "eval_mean_reward, eval_std_reward, evaluate_data = evaluate_agent(eval_env, 20, actor)\n",
    "eval_env.close()\n",
    "print(f\"mean_reward={eval_mean_reward:.2f}\")\n",
    "print(f\"std_reward={eval_std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (F) Save ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(qn.state_dict(), f\"models/{run_name}_critic.pt\")\n",
    "torch.save(actor.state_dict(), f\"models/{run_name}_actor.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Z) Remarks ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "My codes may be wrong, CleanRL's original code exhibits the expected behaviour: learning the local optima (not stepping on the peddle) and never being able to explore the true goal of climbing the mountain. Mine somehow discover the true goal, but the plocy abruptly shifts away (last ep >90 reward, next - 60 and never recover/reach true goal thereafter) at some point. \n",
    "- As always, the DQN family are slow starters. They need, in order, \n",
    "    1 Come across desirable behaviour in exploration\n",
    "    2 Accumulate enough instances of desirable behaviour in its replay memory to make them sample frequently enough\n",
    "    3 Reach `learning_starts`\n",
    "    4 \n",
    "DDPG (I tried the CleanRL original code) performed very badly on MountainCarContinuous, which has a scarce actual reward (destination reached) and an adverse reward (penalize large magnitude actions).\n",
    "- Another problem is that the exploration is ineffective: a mean-zero exploration strategy is a bad one for mountain-car.  It also explores the action space in isolation to the state space.\n",
    "- I observed underestimation of q. The episodic return once reached > 90 and the episodic length < 150. Then q_est should > 0 + 0.99 ** 150 * 90 = 19.9, but q_est never went above 5 during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "actor = Actor(envs).to(device)\n",
    "actor.load_state_dict(torch.load(\"models/Hopper-v4__DDPG__2__0619_1143_actor.pt\"))\n",
    "qn = QNetwork(envs).to(device)\n",
    "qn.load_state_dict(torch.load(\"models/Hopper-v4__DDPG__2__0619_1143_critic.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.reset()\n",
    "envs.step(envs.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "obs, info = envs.reset()\n",
    "actions = envs.action_space.sample()\n",
    "next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "rb.add(obs, next_obs, actions, rewards, terminations, infos) \n",
    "rb.sample(1).observations.dtype\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo8",
   "language": "python",
   "name": "ppo8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
