{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PPO Notebook (StableBaselines3)** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Setups ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B1) Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import (\n",
    "    notebook_login,\n",
    ")\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Implementation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"LunarLander-v2\"\n",
    "env = make_vec_env(env_id, n_envs=16)\n",
    "\"\"\"\n",
    "adding env_kwargs={\"render_mode\": \"human\"} makes the renderer constantly flickering, presumably\n",
    "through the 16 sub-vec_envs\n",
    "\n",
    "Official examples from\n",
    "https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
    "only enjoy the agent AFTER the training\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ppo-LunarLander-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my CleanRL hyperparameters\n",
    "# performs worse than course's hyperparameters: lower mean reward, slower episode/time\n",
    "\"\"\"\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\", \n",
    "    env=env, \n",
    "    learning_rate=0.0003, \n",
    "    n_steps=512, \n",
    "    batch_size=256, \n",
    "    n_epochs=8, \n",
    "    gamma=0.995, \n",
    "    gae_lambda=0.95, \n",
    "    clip_range=0.2, \n",
    "    clip_range_vf=0.2, \n",
    "    ent_coef=0.01, \n",
    "    vf_coef=0.5, \n",
    "    max_grad_norm=0.5, \n",
    "    verbose=1, \n",
    "    tensorboard_log=\"./runs/\"\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# course hyperparameters\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./runs/\", \n",
    ")\n",
    "\"\"\"\n",
    "The author of the course \"got a mean reward of 200.20 +/- 20.80 \n",
    "after training for 1 million steps, which means\" \n",
    "he failed to pass ??!\n",
    "\n",
    "see eval\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Evaluate ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = Monitor(gym.make(env_id, render_mode=\"human\")) # render_mode=\"human\" slows down the eval process (eval finishes only when render finishes)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "eval_env.close() # else render window will not close correctly and force-closing raises an error\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# tensorboard shows ep_rew_mean ~ 150 at the end of training, \n",
    "# evaluate_policy() results in mean_reward=252.16 +/- 24.04\n",
    "# Probably, tensorboard does not add the final reward (+100 for successful landing)\n",
    "# ^ not true, the ep_len_mean hovered just below 1000\n",
    "\n",
    "# Not exactly sure what deterministic=True does \n",
    "# # (in the source code, it calls model.predict() with deterministic=True, but what that \n",
    "# in turn does??)\n",
    "# The rendering shows the agent keeps firing the engines at random after landing, \n",
    "# a behaviour not seen when deterministic=True. It keeps it until truncation\n",
    "# So I conclude that deterministic=True sets the policy to argmax instead of softmax\n",
    "# https://stable-baselines3.readthedocs.io/en/master/common/evaluation.html#module-stable_baselines3.common.evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (E) Save / Load ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo\" + env_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo\" + env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (F) Push ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = \"PPO\"\n",
    "repo_id = \"Rudolph314/sb3ppo-LunarLander-v2\"\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
    "\n",
    "package_to_hub(\n",
    "    model=model,  # Our trained model\n",
    "    model_name=model_name,  # The name of our trained model\n",
    "    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n",
    "    env_id=env_id,  # Name of the environment\n",
    "    eval_env=eval_env,  # Evaluation Environment\n",
    "    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "    commit_message=commit_message,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3",
   "language": "python",
   "name": "sb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
