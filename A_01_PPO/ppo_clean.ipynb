{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PPO Notebook (CleanRL)** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is a combination of CleanRL's [blog post](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) and its accompanying video tutorial, part 1. \n",
    "\n",
    "I have taken out some parts for separate treatment/introduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*renamed variables*\n",
    "+ the plural / singular\n",
    "+ `exp_name` -> `algo`\n",
    "+ `num_steps` -> `train_interval`\n",
    "+ `done` -> `rstt` \n",
    "    + the differnce being if t=8 is done, then t=9 is restarting (rstt) and s_9 is a initial state\n",
    "+ `b_inds` -> `b_idxs`\n",
    "+ `v_loss_unclipped` -> `vunclipped_loss`, `v_loss_clipped` -> `vclipped_loss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (A/B) Set Up ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "# (B2) argparse\n",
    "import os\n",
    "# (B3,4) tensorboard & wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# (C) Implementation\n",
    "# (1) vector environment\n",
    "import gymnasium as gym\n",
    "# (2) agent layer initialization\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B2) args ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(dict): # https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute?page=1&tab=scoredesc#tab-top\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super(Args, self).__init__(*args, **kwargs)\n",
    "            self.__dict__ = self\n",
    "# may reference stablebaselines 3 's hyperparameters\n",
    "# https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "# some are not implemented by me\n",
    "args = {\n",
    "    \"algo\": \"ppo\", \n",
    "    \"env_id\": \"LunarLander-v2\", \n",
    "    \"learning_rate\": 3e-3, # StableBaselines3 uses 3e-3, CleanRL uses 2.5e-4 (too small when I tried)\n",
    "    \"learning_rate_final\": 1e-4, \n",
    "    \"seed\": 2, \n",
    "    \"total_timesteps\": 500_000, # Unit 1 trains for 1e6 total_timestep, might have overfitted\n",
    "    \"capture_video\": True,\n",
    "    \"video_trigger_episode_n\": 20, # dor the 0-th env, so the actual episode_n is this x (num_envs)\n",
    "    # algorithm-specific\n",
    "    \"num_envs\": 16, #4 \n",
    "    \"train_interval\": 512, # should be comparable to episodic length (?)\n",
    "    \"anneal_lr\": True, \n",
    "    \"gamma\": 0.995, \n",
    "    \"lambda_gae\": 0.95, # GAE lambda, controls propagate like TD(λ)\n",
    "    \"clip_coef\": 0.2, # for clipping both policy loss and value loos (detail#8, detail#9)\n",
    "    \"clip_vloss\": False, \n",
    "    \"ent_coef\": 0.01, # weight for entropy, policy loss as 1, during optimization (detail#10)\n",
    "    \"ent_coef_final\": 0.0, \n",
    "    \"vf_coef\": 0.5, # same\n",
    "    \"max_grad_norm\": 0.5, \n",
    "    \"update_epochs\": 8, # train over memory this many times at each iteration\n",
    "    \"norm_adv\": True, # normalize GAE\n",
    "    \n",
    "    \"torch_deterministic\": True, \n",
    "    \"cuda\": True, \n",
    "\n",
    "    # filled in runtime\n",
    "    \"batch_size\": 0, \n",
    "    \"num_minibatches\": 32, \n",
    "    \"minibatch_size\": 0, \n",
    "    \"num_iterations\": 0, \n",
    "}\n",
    "args = Args(args)\n",
    "args.batch_size = int(args.num_envs * args.train_interval)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "args.num_iterations = args.total_timesteps // args.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = datetime.datetime.now().strftime(\"%m%d_%H%M\")\n",
    "run_name = f\"{args.env_id}__{args.algo}__{args.seed}__{start_datetime}\"\n",
    "\n",
    "print(f\"start_datetime = {start_datetime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B3) Hardware ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"device_name = {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B4) Tensorboard ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\", \n",
    "    \"|param|value|\\n|-|-|\\n%s\" % \"\\n\".join(f\"|{key}|{val}\" for key, val in args.items())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B5) seeding ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "random.seed(args[\"seed\"])\n",
    "np.random.seed(args[\"seed\"])\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = args[\"torch_deterministic\"]\n",
    "# episodic rewards, for example, varies slightly during training on Tensorboard, but is identical after training finishes (due to smoothing?)\n",
    "# ^ not really smoothing, upon closer inspection, most are identical, but some are just not ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Implementation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) vector environment ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detail#1\n",
    "def make_env(env_id, idx, capture_video, run_name, episode_trigger_n=args.video_trigger_episode_n):\n",
    "    # idx is the index in the vector environment\n",
    "    # run_name = f'{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}'\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"human\")\n",
    "            env = gym.wrappers.RecordVideo(\n",
    "                env, \n",
    "                f\"videos/{run_name}\", \n",
    "                #episode_trigger=lambda n: n%episode_trigger_n == 0, \n",
    "            )\n",
    "        else: \n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(args.env_id, idx, args.capture_video, run_name) for idx in range(args.num_envs)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1a) test_vec_env ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vec_env(num_envs=4, capture_video=True):\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(\"CartPole-v1\", idx, capture_video, \"test_vec_env\") for idx in range(num_envs)]\n",
    "    )\n",
    "    \n",
    "    global_step = 0\n",
    "    global_episode = 0\n",
    "    \n",
    "    obs = envs.reset()\n",
    "    for _ in range(500):\n",
    "        global_step += num_envs\n",
    "        \n",
    "        actions = envs.action_space.sample()\n",
    "        obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "        \n",
    "        if \"final_info\" in infos:\n",
    "            for idx, info in enumerate(infos[\"final_info\"]): # 'final_info' is an array\n",
    "                if info and \"episode\" in info: # \"episode\" only if info is not None and wrappers.RecordEpisodeStatistics is on\n",
    "                    print(f\"global_episode={global_episode}, env_idx={idx}\")\n",
    "                    print(f\"global_step={global_step}, \\nepisodic_return={info['episode']['r']}\")\n",
    "                    global_episode += 1\n",
    "    envs.close()\n",
    "\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_vec_env(args.num_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) agent -- initialization, layzers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.init https://pytorch.org/docs/stable/nn.init.html\n",
    "# detail#2\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    nn.init.orthogonal_(layer.weight, std)\n",
    "    nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_size = 64\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(\n",
    "                np.array(envs.single_observation_space.shape).prod(), \n",
    "                hidden_size)\n",
    "            ), \n",
    "            nn.Tanh(), \n",
    "            layer_init(nn.Linear(\n",
    "                hidden_size, \n",
    "                hidden_size\n",
    "            )), \n",
    "            nn.Tanh(), \n",
    "            layer_init(nn.Linear(\n",
    "                hidden_size, \n",
    "                1\n",
    "            ), std=1.0) # std=1.0 draws on domain knowledge\n",
    "        )\n",
    "        self.actor = nn.Sequential( # returns not normalized, not exponential 'probabilities'\n",
    "            layer_init(nn.Linear(\n",
    "                np.array(envs.single_observation_space.shape).prod(), \n",
    "                hidden_size, \n",
    "            )), \n",
    "            nn.Tanh(), \n",
    "            layer_init(nn.Linear(\n",
    "                hidden_size, \n",
    "                hidden_size\n",
    "            )), \n",
    "            nn.Tanh(), \n",
    "            layer_init(nn.Linear(\n",
    "                hidden_size, \n",
    "                envs.single_action_space.n # gym.spaces.Discrete\n",
    "            ), std=0.01) # soft-max?\n",
    "        )\n",
    "    \n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    # good practice to bundle actor inference with critic inference\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        logits = self.actor(obs)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(envs).to(device) # ?does that require all return values to be a torch.Tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Adam -- epsilon ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detail#3\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Adam -- anneal learning rate ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(optimizer.param_groups[0])\n",
    "print(optimizer.param_groups[0])\n",
    "\n",
    "# don't really understand between optim.state_dict(), optim.state, and optim.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALGO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALGO - init ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO: PPO's storage\n",
    "# PPO is similar to DQN in that it rollouts for some timesteps and then batch updates with the expereinces thence collected\n",
    "# , therefore it needs storage\n",
    "\n",
    "# another dimension of size train_interval over the dimensions of obs and actions, as they are returned by step()\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        # these memory tensors are overwritten during each rollouts\n",
    "        self.obs = torch.zeros((args.train_interval, args.num_envs) + envs.single_observation_space.shape).to(device) # 128 x 8 x 4\n",
    "        self.actions = torch.zeros((args.train_interval, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "        self.logprobs = torch.zeros((args.train_interval, args.num_envs) + envs.single_action_space.shape).to(device) # they did not have single_action_space.shape; shouldn't this be torch.zeros_like(self.actions)?\n",
    "        self.rewards = torch.zeros((args.train_interval, args.num_envs)).to(device)\n",
    "        self.rstts = torch.zeros((args.train_interval, args.num_envs)).to(device)\n",
    "        self.values = torch.zeros((args.train_interval, args.num_envs)).to(device)\n",
    "\n",
    "memory = Memory()\n",
    "\n",
    "# ALGO: iteration init\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=args.seed)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_rstt = torch.zeros(args.num_envs).to(device)\n",
    "\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALOG - learn the types and dimensions ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "observation_space and action_space inherits from gymnasium.spaces:\n",
    "https://gymnasium.farama.org/api/spaces/#fundamental-spaces\n",
    "isinstance(envs.observation_space, gym.spaces.Space) # True\n",
    "isinstance(envs.observation_space, gym.spaces.Box) # True, 'box' in the sense that it has a upper- and lower- bound in every dimension of the R^n it resides in\n",
    "isinstance(envs.observation_space, np.ndarray) # False\n",
    "isinstance(envs.action_space, gym.spaces.Space) # True\n",
    "isinstance(envs.action_space, gym.spaces.MultiDiscrete) # True, and is not gym.spaces.Discrete\n",
    "isinstance(envs.single_action_space, gym.spaces.Discrete)\n",
    "'''\n",
    "\n",
    "'''\n",
    "print(\"-- Observations --\")\n",
    "# a box has .low, .high, .shape, .sample(), ...\n",
    "envs.single_observation_space.shape                 # (4, )\n",
    "envs.observation_space.shape                        # (4, 4)\n",
    "next_obs                                            # a size([4,4]) tensor\n",
    "\n",
    "print(\"\\n-- Actions --\")\n",
    "# a discrete has n, start, sample(), ...\n",
    "envs.single_action_space                            # Discrete(2), this means the possible actions is the set {0, 1} = {start, start+1, ..., start+n-1}\n",
    "envs.single_action_space.n                          # 2\n",
    "envs.single_action_space.start                      # 0, default\n",
    "envs.action_space                                   # MultiDiscrete([2 2 2 2])\n",
    "envs.action_space.nvec                              # array([2, 2, 2, 2], dtype=int64), IS a np.ndarray\n",
    "## \n",
    "_action, _log_probs, _entropys, _values = agent.get_action_and_value(next_obs)\n",
    "_action                                             # a size([4]) tensor\n",
    "_action.device                                      # device(type='cpu') ???\n",
    "_log_probs                                          # a size([4]) tensor\n",
    "_entropys                                           # a size([4]) tensor\n",
    "_values                                             # a size([4,1]) tensor\n",
    "\n",
    "print(\"\\n-- Rewards --\")\n",
    "next_obs, rewards, terminations, truncations, infos, = envs.step(action.cpu().numpy())\n",
    "rewards                                             # a size([4]) array\n",
    "\n",
    "print(\"\\n-- Values --\")\n",
    "agent.get_value(next_obs)                           # a size([4,1]) tensor\n",
    "\n",
    "# some observations\n",
    "## 1. tensors keep their num_envs(i.e. -2nd) dimension during rollouts, it is flattened only for the iterations/updates\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard and wandb setup ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALGO - loop ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st at rt indexing\n",
    "\n",
    "# loop till ~total_timesteps\n",
    "for updates_n in range(1, args.num_iterations+1):\n",
    "    if args.anneal_lr:\n",
    "        # (4) linear anneal from 1 to 0; detail#4\n",
    "        frac = 1.0 - (updates_n - 1.0) / args.num_iterations\n",
    "        lrnow = frac * args.learning_rate + (1-frac) * args.learning_rate_final\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "        # (my) \n",
    "        ent_coef = frac * args.ent_coef + (1-frac) * args.ent_coef_final\n",
    "\n",
    "    # (A) loop till train_interval\n",
    "    for step in range(0, args.train_interval): # a rollout session of length train_interval\n",
    "        # (A1) next becomes now\n",
    "        global_step += args.num_envs\n",
    "        memory.obs[step] = next_obs\n",
    "        memory.rstts[step] = next_rstt\n",
    "\n",
    "        # (A2) actor-critic inference\n",
    "        with torch.no_grad(): # grad-descent only every num_iterations timesteps\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            memory.values[step] = value.flatten()\n",
    "        memory.actions[step] = action\n",
    "        memory.logprobs[step] = logprob\n",
    "\n",
    "        # (A3) step\n",
    "        next_obs, rewards, terminations, truncations, infos, = envs.step(action.cpu().numpy())\n",
    "        memory.rewards[step] = torch.tensor(rewards).to(device).view(-1)\n",
    "        next_obs = torch.Tensor(next_obs).to(device)\n",
    "        next_rstt = torch.Tensor(np.logical_or(terminations, truncations)).to(device)\n",
    "\n",
    "        # print training progress\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info is None:\n",
    "                    continue\n",
    "                print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "                break # even if two envs are done at the same step, showing one is enough\n",
    "    \n",
    "    # after each training_interval\n",
    "    # (B) calc train_interval advantages and returns\n",
    "    with torch.no_grad():\n",
    "        # a backward td(λ) propagation of advantage. \n",
    "        # But how can you compare? \n",
    "        # A -100 next-state is good because a -10 action was picked, a +100 state is shunned because +10 action was picked. Because action converges to greedy? \n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(memory.rewards).to(device)\n",
    "        lastgaelam = 0 # extremely confusing name, I bet it means gae_t+1 times gaelambda, undiscounted\n",
    "        for t in reversed(range(args.train_interval)):\n",
    "            # (1) resets eligibility trace if t+1 is a reset state\n",
    "            if t == args.train_interval - 1:\n",
    "                nextvalue = next_value\n",
    "                nextnonreset = 1.0 - next_rstt\n",
    "            else:\n",
    "                nextvalue = memory.values[t+1]\n",
    "                nextnonreset = 1.0 - memory.rstts[t+1]\n",
    "            # (2)\n",
    "            delta = memory.rewards[t] + nextnonreset * args.gamma * nextvalue - memory.values[t] # plain δ = rt + γ v_t+1 - vt = qt - vt = advantage\n",
    "            advantages[t] = lastgaelam = delta + \\\n",
    "                    nextnonreset * args.gamma * args.lambda_gae * lastgaelam\n",
    "    returns = advantages + memory.values\n",
    "    \n",
    "    # (C) actor-critic updates\n",
    "    # (C1) flatten out the envs dimensions to the outermost dimension ('b' for batch)\n",
    "    b_obs = memory.obs.reshape((-1,) + envs.single_observation_space.shape) # rehsape to (-1, 4)\n",
    "    b_actions = memory.actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_logprobs = memory.logprobs.reshape((-1,) + envs.single_action_space.shape) # again should share b_actions shape\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = memory.values.reshape(-1)\n",
    "\n",
    "    # (C2) minibatch updates; detail#6\n",
    "    b_idxs = np.arange(args.batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(b_idxs)\n",
    "        for start_idx in range(0, args.batch_size, args.minibatch_size):\n",
    "            # ( ) prob ratio\n",
    "            mb_idxs = b_idxs[start_idx:start_idx+args.minibatch_size] # indices of memory used in this minibatch\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n",
    "                b_obs[mb_idxs], b_actions.long()[mb_idxs] # why long()=to(int64)?\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[mb_idxs] # the 0th minibatch's newlogprob and b_logprob are the same, each optimizer.step() advances the actor-crtic away from memory.logprobs\n",
    "            ratio = logratio.exp() # prob ratio, \n",
    "\n",
    "            # debug variables; detail#12\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()] # fraction in batch whose pg_loss will be clipped, list additon concats, then averaged in plot progress\n",
    "\n",
    "\n",
    "            # ( ) advantage normalization, detail#7, minibatch-level\n",
    "            mb_advantages = b_advantages[mb_idxs]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # ( ) clip policy loss, detail#8\n",
    "            pg_loss1 = -mb_advantages * ratio # the surrogate advantage used by TRPO; TRPO uses a KL-divergence constraint, PPO uses clipping instead (detail#9)\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1-args.clip_coef, 1+args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean() # pg_loss is -ve\n",
    "\n",
    "            # ( ) clip value loss, detail#9\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                vunclipped_loss = (newvalue - b_returns[mb_idxs]) ** 2 # newvalue is from agent.critic, b_returns is from train interval GAE\n",
    "                newvalue_clipped = b_values[mb_idxs] + torch.clamp( # newvalue clipped towards memory.values\n",
    "                    newvalue - b_values[mb_idxs], \n",
    "                    -args.clip_coef, \n",
    "                    args.clip_coef, \n",
    "                )\n",
    "                vclipped_loss = (newvalue_clipped - b_returns[mb_idxs]) ** 2\n",
    "                v_loss_max = torch.max(vunclipped_loss, vclipped_loss)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_idxs]) ** 2).mean()\n",
    "\n",
    "            # ( ) entropy loss, detail#10\n",
    "            # minimize policy loss and value loss (improve actor and critic) \n",
    "            # but maximize entropy loss (i.e. try to make π(a) even over a to encourage exploration)\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "            # ( ) backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # clip_grad_norm_ rescale all gradient steps by the same ratio such that the sum of the norm of all steps equals to  max_norm, i.e. the 2nd argumant\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm) # global gradient clipping, detail#11\n",
    "            optimizer.step()\n",
    "\n",
    "    # debug variable; detail#12\n",
    "    y_pred = b_values.cpu().numpy() # critic's prediction (one step TD look-ahead) during rollout\n",
    "    y_true = b_returns.cpu().numpy() # based on GAE calculated from train interval trajectories, confusing name\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # progress plots\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step) # is mean-square-error\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step) # ~ how much the new policy fares better than the previous one\n",
    "    writer.add_scalar(\"losses/entropy_loss\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step) # how many oversized policy gradient step prevented\n",
    "    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step) # ~ how well critic is doing\n",
    "\n",
    "experiment_duration = time.time() - start_time\n",
    "print(f\"global_step = {global_step}\")\n",
    "print(f\"experiment_duration = {experiment_duration:.2f} seconds\")\n",
    "writer.add_text(\"experiment_duration\", f\"{experiment_duration:.2f}\", global_step)\n",
    "envs.close()\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Evaluation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import json\n",
    "import shutil\n",
    "import imageio\n",
    "\n",
    "from wasabi import Printer\n",
    "\n",
    "msg = Printer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we evaluate the agent first because \n",
    "(a) The course's record video function often crashes\n",
    "(b) RecordVideo does not allow fully custom video naming\n",
    "(c) Technically you should evaluate, push or not\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_agent(env, n_eval_episodes, policy, hyperparameters=args):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param policy: The agent\n",
    "    \"\"\"\n",
    "    # (1) evaluate\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        while done is False:\n",
    "            state = torch.Tensor(state).to(device)\n",
    "            action, _, _, _ = policy.get_action_and_value(state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "            total_rewards_ep += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "        print(f\"episode {episode:2}: reward={total_rewards_ep:5.1f}\")\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    # (2) metadata\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "    evaluate_data = {\n",
    "        \"env_id\": hyperparameters.env_id,\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"std_reward\": std_reward,\n",
    "        \"n_evaluation_episodes\": n_eval_episodes,\n",
    "        \"eval_datetime\": eval_form_datetime,\n",
    "    }\n",
    "\n",
    "    return mean_reward, std_reward, evaluate_data\n",
    "\n",
    "eval_env = gym.make(args.env_id, render_mode=\"rgb_array\")\n",
    "eval_env = gym.wrappers.RecordVideo(eval_env, video_folder=\"upload\", step_trigger=lambda x: x==0, video_length=1000) # 1000=20sec (not 30FPS?)\n",
    "\n",
    "eval_mean_reward, eval_std_reward, evaluate_data = evaluate_agent(eval_env, 10, agent)\n",
    "print(f\"mean_reward={eval_mean_reward:.2f}\")\n",
    "print(f\"std_reward={eval_std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (E) Push ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_to_hub(\n",
    "    repo_id,\n",
    "    model,\n",
    "    hyperparameters,\n",
    "    commit_message=\"Push agent to the Hub\",\n",
    "    token=None,\n",
    "    logs=None,\n",
    "):\n",
    "    msg.info(\n",
    "        \"This function will save, evaluate, generate a video of your agent, create a model card and push everything to the hub. \"\n",
    "    )\n",
    "\n",
    "    # convert to same API as original code\n",
    "    mean_reward = eval_mean_reward\n",
    "    std_reward = eval_std_reward \n",
    "\n",
    "    # Step 1: Clone or create the repo\n",
    "    repo_url = HfApi().create_repo(\n",
    "        repo_id=repo_id,\n",
    "        token=token,\n",
    "        private=False,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    # Step 2: Save the model\n",
    "    dirname = \"upload\"\n",
    "    path = Path(dirname)\n",
    "    torch.save(model.state_dict(), path / \"model.pt\")\n",
    "\n",
    "    # Step 3: Get evaluation results (eval_mean_reward, eval_std_reward, evaluate_data)\n",
    "    print(f\"mean_reward={mean_reward:.2f}\")\n",
    "    print(f\"std_reward={std_reward:.2f}\")\n",
    "    # Step 3a: write to JSON\n",
    "    with open(path / \"results.json\", \"w\") as outfile:\n",
    "            json.dump(evaluate_data, outfile)\n",
    "    \n",
    "    # Step 4: Rename the video (evaluate_agent generates video as \"upload/rl-video-step-0.mp4\")\n",
    "    if os.path.isfile(path / \"rl-video-step-0.mp4\"):\n",
    "        os.rename(path / \"rl-video-step-0.mp4\", path / \"replay.mp4\")\n",
    "    else:\n",
    "        print(\"Warning: Using existing replay.mp4\")\n",
    "    \n",
    "    # Step 5: Generate the model card\n",
    "    generated_model_card, metadata = _generate_model_card(\n",
    "        \"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters\n",
    "    )\n",
    "    _save_model_card(path, generated_model_card, metadata)\n",
    "\n",
    "    # Step 6: Add logs if needed\n",
    "    if logs:\n",
    "        _add_logdir(path, Path(logs))\n",
    "\n",
    "    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n",
    "    # Step 7: Push\n",
    "    repo_url = upload_folder(\n",
    "            repo_id=repo_id,\n",
    "            folder_path=path,\n",
    "            path_in_repo=\"\",\n",
    "            commit_message=commit_message,\n",
    "            token=token,\n",
    "        )\n",
    "\n",
    "    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n",
    "    \n",
    "    return repo_url\n",
    "\n",
    "\n",
    "def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n",
    "    \"\"\"\n",
    "    Generate the model card for the Hub\n",
    "    :param model_name: name of the model\n",
    "    :env_id: name of the environment\n",
    "    :mean_reward: mean reward of the agent\n",
    "    :std_reward: standard deviation of the mean reward of the agent\n",
    "    :hyperparameters: training arguments\n",
    "    \"\"\"\n",
    "    # Step 1: Select the tags\n",
    "    metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n",
    "\n",
    "    # Transform the hyperparams namespace to string\n",
    "    converted_dict = vars(hyperparameters)\n",
    "    converted_str = str(converted_dict)\n",
    "    converted_str = converted_str.split(\", \")\n",
    "    converted_str = \"\\n\".join(converted_str)\n",
    "\n",
    "    # Step 2: Generate the model card\n",
    "    model_card = f\"\"\"\n",
    "  # PPO Agent Playing {env_id}\n",
    "\n",
    "  This is a trained model of a PPO agent playing {env_id}.\n",
    "\n",
    "  # Hyperparameters\n",
    "  \"\"\"\n",
    "    return model_card, metadata\n",
    "\n",
    "def generate_metadata(model_name, env_id, mean_reward, std_reward):\n",
    "    \"\"\"\n",
    "    Define the tags for the model card\n",
    "    :param model_name: name of the model\n",
    "    :param env_id: name of the environment\n",
    "    :mean_reward: mean reward of the agent\n",
    "    :std_reward: standard deviation of the mean reward of the agent\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [\n",
    "        env_id,\n",
    "        \"ppo\",\n",
    "        \"deep-reinforcement-learning\",\n",
    "        \"reinforcement-learning\",\n",
    "        \"custom-implementation\",\n",
    "        \"deep-rl-course\",\n",
    "    ]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=model_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_id,\n",
    "        dataset_id=env_id,\n",
    "    )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def _save_model_card(local_path, generated_model_card, metadata):\n",
    "    \"\"\"Saves a model card for the repository.\n",
    "    :param local_path: repository directory\n",
    "    :param generated_model_card: model card generated by _generate_model_card()\n",
    "    :param metadata: metadata\n",
    "    \"\"\"\n",
    "    readme_path = local_path / \"README.md\"\n",
    "    readme = \"\"\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = generated_model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "\n",
    "def _add_logdir(local_path: Path, logdir: Path):\n",
    "    \"\"\"Adds a logdir to the repository.\n",
    "    :param local_path: repository directory\n",
    "    :param logdir: logdir directory\n",
    "    \"\"\"\n",
    "    if logdir.exists() and logdir.is_dir():\n",
    "        # Add the logdir to the repository under new dir called logs\n",
    "        repo_logdir = local_path / \"logs\"\n",
    "\n",
    "        # Delete current logs if they exist\n",
    "        if repo_logdir.exists():\n",
    "            shutil.rmtree(repo_logdir)\n",
    "\n",
    "        # Copy logdir into repo logdir\n",
    "        shutil.copytree(logdir, repo_logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_repo_id = \"Rudolph314/ppo-LunarLander-v2\"\n",
    "\n",
    "package_to_hub(\n",
    "    repo_id=hub_repo_id,\n",
    "    model=agent,\n",
    "    hyperparameters=args,\n",
    "    commit_message=\"modified from CleanRL\",\n",
    "    logs=f\"runs/eval_{run_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (F) Save ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(), f\"models/{run_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (G) Remarks ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that landing outside of the pad is legit although the rough terrain there makes it difficult. ~~Perhaps because of this, the agent sees a huge advantage and often ventures there~~. Sometimes they don't crash but instead tries to stabilize itself with thrusters while sliding / rotate when tilted (this situation also happens when one feet is on the pad). See also\n",
    "https://github.com/DLR-RM/stable-baselines3/issues/1669\n",
    "\n",
    "The default ent_coef is 1e-2 and the tyical value of entropy_loss is 1. I observed that the policy_loss of LunarLander is ~ 1e-2 to 1e-3, wouldn't the entropy be too large??\n",
    "\n",
    "The video's learning_rate, num_envs, train_interval, epochs, num_minibatches as a whole is fairly expereience-inefficient. Many problems / local optima can be solved by raising them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanrl",
   "language": "python",
   "name": "cleanrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
