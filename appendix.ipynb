{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iPyNb has support for MathJax. <br>\n",
    "Quick reference on [StackExchange](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (R) Articles ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TRPO](https://arxiv.org/abs/1502.05477) (2015) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eq.1**\n",
    "\n",
    "$$\n",
    "\\eta(\\tilde{\\pi}) = \\eta(\\pi) + \\mathbb{E}_{\\tau\\sim\\tilde{\\pi}} [ \\sum_{t=0}^\\infty \\gamma^t A_\\pi(s_t, a_t) ]\n",
    "$$\n",
    "\n",
    "The expected discounted return of a policy $\\eta(\\tilde{\\pi})$ can be evaluated from the return of another policy $\\eta(\\pi)$. \n",
    "- $\\mathbb{E}_{\\tau\\sim\\tilde{\\pi}}$ is the expected return of trajectories generated by policy $\\pi$. \n",
    "- $A_\\pi(s_t, a_t)$ is the advantage of taking action $a_t$ at state $s_t$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Eq.2**\n",
    "\n",
    "$$\n",
    "\\eta(\\tilde{\\pi}) = \\eta(\\pi) + \\sum_{s} \\rho_{\\tilde{\\pi}(s)} \\sum_{a} \\tilde{\\pi}(a|s) A_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "The expected discounted return of a policy $\\eta(\\tilde{\\pi})$ can be written as a sum over states instead of over timesteps. \n",
    "It also implies that the new policy $\\tilde{\\pi}$ is guaranteed to be an improvement if for every state (reachable by $\\tilde{\\pi}$), $\\sum_{a} \\tilde{\\pi}(a|s) A_\\pi(s,a) \\ge 0$. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Eq.3**\n",
    "\n",
    "$$\n",
    "L_\\pi(\\tilde{\\pi}) = \\eta(\\pi) + \\sum_{s} \\rho_{\\pi(s)} \\sum_{a} \\tilde{\\pi}(a|s) A_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "$L_\\pi(\\tilde{\\pi})$, $L_\\pi$ for short, approximates $\\eta(\\tilde{\\pi})$ by using the visitation density $\\rho_{\\pi(s)}$ instead of $\\rho_{\\tilde{\\pi}(s)}$, on the condition that the improvement from $\\pi$ to $\\tilde{\\pi}$ is small. The article proceeds to discuss the extent to which the approximation is valid. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Eq.14a**\n",
    "$$\n",
    "\\underset{\\theta}{\\mathrm{maximize}} \\, \\mathbb{E}_{s\\sim\\rho_\\pi, a\\sim q} [\\frac{\\tilde{\\pi}_(a|s)}{q(a|s)} A_\\pi(s,a)]\n",
    "$$\n",
    "*notice the change in notations in the original article, which happened in setion 4 \"we will overload our previous notation to use functions ...\"*\n",
    "<br>\n",
    ", where $q(a|s)$ is just any distribution function of actions $a$. We multiply $\\frac{q(a|s)}{q(a|s)}$ into the inner summation of **Eq.3** to transform the summation into an expectation. In particular, we can use the old distribution/policy $\\pi(a|s)$ as the distribution function.\n",
    "Furthermore, realizing $V(s)$ being a constant, one interchange between $A(a, s)$ and $Q(a,s)$ in the maximization to obtain **Eq.14a**, giving \n",
    "\n",
    "**Eq.14b**\n",
    "$$\n",
    "\\underset{\\theta}{\\mathrm{maximize}} \\, \\mathbb{E}_{s\\sim\\rho_\\pi, a\\sim\\pi} [\\frac{\\tilde{\\pi}_(a|s)}{\\pi(a|s)} Q_\\pi(s,a)]\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
